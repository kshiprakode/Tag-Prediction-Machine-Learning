{"posts": {"#text": "\n  ", "row": [{"@Body": "<p>The set difference operator (e.g., <code>EXCEPT</code> in some SQL variants) is one of the many fundamental operators of relational algebra. However, there are some databases that do not support the set difference operator directly, but which support <code>LEFT JOIN</code> (a kind of outer join), and in practice this can be used instead of a set difference operation to achieve the same effect.</p>\n\n<p>Does this mean that the expressive power of a query language is the same even without the set difference operator, so long as the <code>LEFT JOIN</code> operator is maintained? How would one prove this fact?</p>\n", "@ViewCount": "416", "#tail": "\n  ", "@Title": "Does the 'difference' operation add expressiveness to a query language that already includes 'join'?", "@Tags": "<database-theory><relational-algebra><finite-model-theory>", "@LastEditorUserId": "69", "@LastActivityDate": "2013-05-29T00:50:34.590", "@LastEditDate": "2012-04-02T15:35:05.827", "@Score": "16", "@CommentCount": "1", "@AcceptedAnswerId": "28", "@PostTypeId": "1", "@AnswerCount": "2", "@CreationDate": "2012-03-06T19:06:05.667", "@FavoriteCount": "1", "@Id": "2", "@OwnerUserId": "5"}, {"@Body": "<p>In a standard algorithms course we are taught that <strong>quicksort</strong> is $O(n \\log n)$ on average and $O(n^2)$ in the worst case. At the same time, other sorting algorithms are studied which are $O(n \\log n)$ in the worst case (like <strong>mergesort</strong> and <strong>heapsort</strong>), and even linear time in the best case (like <strong>bubblesort</strong>) but with some additional needs of memory.</p>\n\n<p>After a quick glance at <a href=\"http://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms\">some more running times</a> it is natural to say that quicksort <strong>should not</strong> be as efficient as others.</p>\n\n<p>Also, consider that students learn in basic programming courses that recursion is not really good in general because it could use too much memory, etc. Therefore (and even though this is not a real argument), this gives the idea that quicksort might not be really good because it is a recursive algorithm.</p>\n\n<p><strong>Why, then, does quicksort outperform other sorting algorithms in practice?</strong> Does it have to do with the structure of <em>real-world data</em>? Does it have to do with the way memory works in computers? I know that some memories are way faster than others, but I don't know if that's the real reason for this counter-intuitive performance (when compared to theoretical estimates).</p>\n\n<hr>\n\n<p><strong>Update 1:</strong> a canonical answer is saying that the constants involved in the $O(n\\log n)$ of the average case are smaller than the constants involved in other $O(n\\log n)$ algorithms. However, I have yet to see a proper justification of this, with precise calculations instead of intuitive ideas only.</p>\n\n<p>In any case, it seems like the real difference occurs, as some answers suggest, at memory level, where implementations take advantage of the internal structure of computers, using, for example, that cache memory is faster than RAM. The discussion is already interesting, but I'd still like to see more detail with respect to memory-management, since it appears that <em>the</em> answer has to do with it.</p>\n\n<hr>\n\n<p><strong>Update 2:</strong> There are several web pages offering a comparison of sorting algorithms, some fancier than others (most notably <a href=\"http://www.sorting-algorithms.com/\">sorting-algorithms.com</a>). Other than presenting a nice visual aid, this approach does not answer my question.</p>\n", "@ViewCount": "172527", "#tail": "\n  ", "@Title": "Why is quicksort better than other sorting algorithms in practice?", "@Tags": "<algorithms><sorting>", "@LastEditorUserId": "24", "@LastActivityDate": "2016-07-19T15:28:19.847", "@LastEditDate": "2012-03-23T13:15:39.290", "@Score": "187", "@CommentCount": "13", "@AcceptedAnswerId": "90", "@PostTypeId": "1", "@AnswerCount": "11", "@CreationDate": "2012-03-06T19:11:07.127", "@FavoriteCount": "123", "@Id": "3", "@OwnerUserId": "24"}, {"@Body": "<p>Many operating systems references say that with cooperative (as opposed to preemptive) multitasking, a process keeps the CPU until it explicitly voluntarily suspends itself.  If a running process performs an I/O request that cannot be immediately satisfied (e.g., requests a key stroke that is not yet available), does the scheduler suspend it, or does it really keep the CPU until the request can be serviced?</p>\n\n<p>[Edited to replace \"blocks on i/o\" with \"performs an I/O request that cannot be immediately satisfied.\"]</p>\n", "@ViewCount": "1145", "#tail": "\n  ", "@Title": "Does cooperative scheduling suspend processes when they perform an  I/O operation?", "@Tags": "<operating-systems><process-scheduling>", "@LastEditorUserId": "98", "@LastActivityDate": "2012-04-09T22:15:05.277", "@LastEditDate": "2012-04-07T13:42:43.093", "@Score": "17", "@CommentCount": "2", "@AcceptedAnswerId": "12", "@PostTypeId": "1", "@AnswerCount": "4", "@CreationDate": "2012-03-06T19:17:48.460", "@FavoriteCount": "0", "@Id": "5", "@OwnerUserId": "40"}, {"@Body": "<p>In comparison to other comparison-based sorting algorithms with $O(n \\lg n)$ time complexity, quick-sort is often considered to better than other algorithms like merge-sort because it is an in-place sorting algorithm. In other words, we don't need (much more) memory to store the members of the array.</p>\n\n<p>ps: to be precise, being better than other algorithms is task dependent. For some tasks it might be better to use other sorting algorithms.</p>\n\n<p>See also:</p>\n\n<ul>\n<li><p><a href=\"http://en.wikipedia.org/wiki/Quicksort#Comparison_with_other_sorting_algorithms\">Comparison of quick-sort with other sorting algorithms</a></p></li>\n<li><p><a href=\"http://en.wikipedia.org/wiki/Heapsort#Comparison_with_other_sorts\">Comparison of heap-sort with other sorting algorithms</a></p></li>\n</ul>\n", "#tail": "\n  ", "@ParentId": "3", "@LastEditorUserId": "41", "@LastActivityDate": "2012-03-06T22:18:33.027", "@LastEditDate": "2012-03-06T22:18:33.027", "@Score": "15", "@CommentCount": "13", "@PostTypeId": "2", "@CreationDate": "2012-03-06T19:19:20.237", "@Id": "6", "@OwnerUserId": "41"}, {"@Body": "<p>When placing geometric objects in a quadtree (or octree), you can place objects that are larger than a single node in a few ways:</p>\n\n<ol>\n<li>Placing the object's reference in every leaf for which it is contained</li>\n<li>Placing the object's reference in the deepest node for which it is fully contained</li>\n<li>Both #1 and #2</li>\n</ol>\n\n<p>For example:</p>\n\n<p><img src=\"http://i.stack.imgur.com/Z2Bj7.jpg\" alt=\"enter image description here\"></p>\n\n<p>In this image, you could either place the circle in all four of the leaf nodes (method #1) or in just the root node (method #2) or both (method #3).</p>\n\n<p>For the purposes of querying the quadtree, which method is more commonplace and why?</p>\n", "@ViewCount": "149", "#tail": "\n  ", "@Title": "Which method is preferred for storing large geometric objects in a quadtree?", "@Tags": "<graphics><data-structures><computational-geometry>", "@LastEditorUserId": "11", "@LastActivityDate": "2012-03-06T20:22:05.510", "@LastEditDate": "2012-03-06T19:47:07.427", "@Score": "14", "@CommentCount": "5", "@PostTypeId": "1", "@AnswerCount": "2", "@CreationDate": "2012-03-06T19:34:22.793", "@Id": "7", "@OwnerUserId": "11"}, {"@Body": "<p>Assuming you are storing a reference, not the object itself, it may make sense to do this differently depending on your application.</p>\n\n<p>For instance, if you were computing collisions with this (solid) circle and the collision was occurring in the lower left hand corner, it'd be easier if you had access to all geometry in that leaf directly from that leaf (method #3) (without having to traverse the tree upward and determine inherited geometry). But, say you were just using quadtrees for drawing geometry, you'd want to use method #1, because it only makes sense to draw something in the node for which it is fully contained (it would be more difficult to figure out which portion to draw for each leaf node and where).</p>\n\n<p>As for what is more commonplace, my only experience with quadtrees is with writing an n-body simulation where the geometric objects were really just points that had no area, so I can't definitively answer that.</p>\n", "@PostTypeId": "2", "#tail": "\n  ", "@LastActivityDate": "2012-03-06T19:44:17.600", "@Score": "6", "@CommentCount": "1", "@ParentId": "7", "@CreationDate": "2012-03-06T19:44:17.600", "@Id": "9", "@OwnerUserId": "44"}, {"@Body": "<p>I think one of the main reasons why QuickSort is so fast compared with other sorting algorithms is because it's cache-friendly. When QS processes a segment of an array, it accesses elements at the beginning and end of the segment, and moves towards the center of the segment.</p>\n\n<p>So, when you start, you access the first element in the array and a piece of memory (\u201clocation\u201d) is loaded into the cache. And when you try to access the second element, it's (most likely) already in the cache, so it's very fast.</p>\n\n<p>Other algorithms like heapsort don't work like this, they jump in the array a lot, which makes them slower.</p>\n", "@PostTypeId": "2", "#tail": "\n  ", "@LastActivityDate": "2012-03-06T19:48:00.343", "@Score": "26", "@CommentCount": "6", "@ParentId": "3", "@CreationDate": "2012-03-06T19:48:00.343", "@Id": "10", "@OwnerUserId": "29"}, {"@Body": "<p>I have a set of pairs. Each pair is of the form (x,y) such that x,y belong to integers from the range <code>[0,n)</code>.</p>\n\n<p>So, if the n is 4, then I have the following pairs:</p>\n\n<pre><code>(0,1) (0,2) (0,3)\n(1,2) (1,3) \n(2,3) \n</code></pre>\n\n<p>I already have the pairs. Now, I have to build a combination using <code>n/2</code> pairs such that none of the integers are repeated (in other words, each integer appears at least once in the final combination). Following are the examples of a correct and an incorrect combination for better understanding</p>\n\n<pre><code> 1. (0,1)(1,2) [Invalid as 3 does not occur anywhere]\n 2. (0,2)(1,3) [Correct]\n 3. (1,3)(0,2) [Same as 2]\n</code></pre>\n\n<p>Can someone suggest me a way to generate all possible combinations, once I have the pairs.</p>\n", "@ViewCount": "4526", "#tail": "\n  ", "@Title": "Generating Combinations from a set of pairs without repetition of elements", "@Tags": "<algorithms>", "@LastEditorUserId": "98", "@LastActivityDate": "2012-03-07T15:45:01.743", "@LastEditDate": "2012-03-07T14:20:14.907", "@Score": "22", "@CommentCount": "8", "@AcceptedAnswerId": "18", "@PostTypeId": "1", "@AnswerCount": "6", "@CreationDate": "2012-03-06T19:54:40.243", "@FavoriteCount": "3", "@Id": "11", "@OwnerUserId": "59"}, {"@Body": "<p>In a truly \"cooperative\" setting, and if there was no hardware protection, a process could certainly block on I/O and not relinquish control until the I/O was done (or never relinquish control at all). For example, Windows 3.1 was this way: if a single user process wanted to take over the entire computer, and prevent anything else from running, it could. </p>\n\n<p>But on a system with multitasking you expect the system API I/O commands to relinquish control of the processor when they are called. So when a running process blocks on I/O, assuming that the process uses the normal system APIs, other processes will be allowed to run until the I/O is complete, and eventually the original process will resume once the I/O is done. In other words, calling a blocking I/O function is one way that a process on a cooperative system can voluntarily suspend itself. </p>\n", "@PostTypeId": "2", "#tail": "\n  ", "@LastActivityDate": "2012-03-06T19:54:59.633", "@Score": "14", "@CommentCount": "0", "@ParentId": "5", "@CreationDate": "2012-03-06T19:54:59.633", "@Id": "12", "@OwnerUserId": "43"}, {"@Body": "<blockquote>\n  <p>If a running process blocks on i/o</p>\n</blockquote>\n\n<p>Blocking on IO is pretty much equivalent to suspending your process. In the context of the linux kernel, executing some IO system call such as <code>read()</code> will cause a <code>sysenter</code> or interrupt handler to trigger to look after that IO, calling <code>do_sys_read()</code> ultimately. Here, if the current request cannot immediately be satisfied, the function calls <code>sched()</code> which then may execute another process.</p>\n\n<p>In the context of a co-operative system, I would expect that when you make a system call for some IO reason, if the request cannot be satisfied the kernel picks another task and executes that. <a href=\"http://myweb.lmu.edu/dondi/share/os/cpu-scheduling.pdf\">This document</a> provides some background - basically, if you waited on IO, you could be hung forever waiting for that IO. The idea of co-operative scheduling being that you frequently call <code>sched()</code> or the equvalent relinquish-the-cpu method, if doing CPU-intensive tasks.</p>\n\n<p>Kernel-mode considerations get more interesting. On architectures where they are available such as <a href=\"http://www.on-time.com/rtos-32-docs/rtkernel-32/programming-manual/advanced-topics/preemptive-or-cooperative-multitasking.htm\">certain embedded platforms</a>, interrupts handlers will still be invoked in response to hardware or software interrupts. It is usually possible, implementation-wise, to <a href=\"http://www.kernelfaq.com/2007/07/enabling-and-disabling-interrupts.html\">disable</a> <a href=\"http://www.xml.com/ldd/chapter/book/ch09.html#t1\">interrupt handling</a>, but that also has drawbacks.</p>\n", "@ParentId": "5", "#tail": "\n\n", "@LastActivityDate": "2012-03-06T19:57:40.567", "@Score": "10", "@CommentCount": "0", "@OwnerDisplayName": "user54", "@PostTypeId": "2", "@CreationDate": "2012-03-06T19:57:40.567", "@Id": "13"}]}}